name: Tourism Project Pipeline

on:
  push:
    branches:
      - main  # Trigger on push to main

env:
  PYTHON_VERSION: "3.11"
  DATASET_REPO: "Yashwanthsairam/tourism-prediction-package"         # <-- change to your HF dataset repo
  MODEL_REPO:   "Yashwanthsairam/tourism-prediction-package"     # <-- change to your HF model repo
  SPACE_REPO:   "Yashwanthsairam/tourism-prediction-package"        # <-- change to your HF Space (streamlit) repo

jobs:

  register-dataset:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas huggingface_hub

      - name: Upload Dataset to Hugging Face Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os
          from huggingface_hub import HfApi, create_repo
          from huggingface_hub.utils import RepositoryNotFoundError

          hf_token = os.environ["HF_TOKEN"]
          repo_id = os.environ["DATASET_REPO"]
          api = HfApi(token=hf_token)

          # Ensure dataset repo exists
          try:
              api.repo_info(repo_id=repo_id, repo_type="dataset")
              print(f"Dataset repo '{repo_id}' exists.")
          except RepositoryNotFoundError:
              print(f"Creating dataset repo '{repo_id}'...")
              create_repo(repo_id=repo_id, repo_type="dataset", private=False)

          # Upload CSV from the repo (adjust path if different)
          local_path = "tourism_project/data/tourism.csv"
          if not os.path.exists(local_path):
              raise FileNotFoundError(f"Could not find dataset at {local_path}. Commit it before running the workflow.")

          api.upload_file(
              path_or_fileobj=local_path,
              path_in_repo="tourism.csv",
              repo_id=repo_id,
              repo_type="dataset",
          )
          print("Dataset upload complete.")
          PY

  data-prep:
    needs: register-dataset
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas scikit-learn datasets huggingface_hub

      - name: Run Data Preparation (split & push X/y to HF Datasets)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os
          import pandas as pd
          from sklearn.model_selection import train_test_split
          from huggingface_hub import HfApi

          repo_id = os.environ["DATASET_REPO"]
          api = HfApi(token=os.environ["HF_TOKEN"])

          # Download back the uploaded CSV (or read from repo working dir if you prefer)
          df = pd.read_csv("tourism_project/data/tourism.csv")

          # Basic clean-up consistent with training
          for col in ["CustomerID", "Unnamed: 0", "Unnamed:0"]:
              if col in df.columns:
                  df.drop(columns=[col], inplace=True)

          target = "ProdTaken"
          if target not in df.columns:
              raise KeyError(f"Target column '{target}' not found in dataset.")

          X = df.drop(columns=[target])
          y = df[target].astype(int)

          Xtrain, Xtest, ytrain, ytest = train_test_split(
              X, y, test_size=0.2, random_state=42, stratify=y
          )

          os.makedirs("prepped", exist_ok=True)
          Xtrain.to_csv("prepped/Xtrain.csv", index=False)
          Xtest.to_csv("prepped/Xtest.csv", index=False)
          ytrain.to_csv("prepped/ytrain.csv", index=False)
          ytest.to_csv("prepped/ytest.csv", index=False)

          # Push splits to the same dataset repo
          for f in ["Xtrain.csv", "Xtest.csv", "ytrain.csv", "ytest.csv"]:
              api.upload_file(
                  path_or_fileobj=f"prepped/{f}",
                  path_in_repo=f,
                  repo_id=repo_id,
                  repo_type="dataset",
              )
              print(f"Uploaded {f}")
          PY

  model-training:
    needs: data-prep
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tourism_project/requirements.txt

      # Avoid starting MLflow UI in CI unless you truly need it.
      # If you have a remote tracking server, set MLFLOW_TRACKING_URI via secrets.
      - name: Model Building (train)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          MLFLOW_TRACKING_URI: file:./mlruns  # optional; if unset, MLflow will use local ./mlruns
        run: |
          python tourism_project/model_building/train.py

      - name: Upload model artifact to GitHub Actions
        uses: actions/upload-artifact@v4
        with:
          name: tourism-model-artifacts
          path: artifacts/**

  deploy-hosting:
    needs: [model-training, data-prep, register-dataset]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install huggingface_hub

      - name: Push files to Frontend Hugging Face Space (Streamlit)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python - <<'PY'
          import os
          from huggingface_hub import HfApi, create_repo
          from huggingface_hub.utils import RepositoryNotFoundError

          api = HfApi(token=os.environ["HF_TOKEN"])
          space_id = os.environ["SPACE_REPO"]

          # Ensure Space exists (sdk=streamlit)
          try:
              api.repo_info(repo_id=space_id, repo_type="space")
              print(f"Space '{space_id}' exists.")
          except RepositoryNotFoundError:
              print(f"Creating Space '{space_id}' (streamlit)...")
              create_repo(repo_id=space_id, repo_type="space", private=False, space_sdk="streamlit")

          # Upload deployment app and requirements
          files = [
              ("tourism_project/deployment/app.py", "app.py"),
              ("tourism_project/deployment/requirements.txt", "requirements.txt"),
          ]
          for local, dest in files:
              if not os.path.exists(local):
                  raise FileNotFoundError(f"Missing file: {local}")
              api.upload_file(
                  path_or_fileobj=local,
                  path_in_repo=dest,
                  repo_id=space_id,
                  repo_type="space",
              )
              print(f"Uploaded {local} -> {space_id}:{dest}")
          PY
